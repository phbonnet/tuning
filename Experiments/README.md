
## Overview ##
   * It is possible to run an experiment or a suite of experiments 
   * It takes one command to run an experiment or a suite of experiments
   * It takes another command to generate the graphs of an experiment
   * For each experiment, a set of files are generated with raw data
   * those raw data are processed into the graph input files
   * For each graph, there is a graph layout file
   * Experiments and graphs are created from OSX command line _with gnuplot_
   * To each experiment is associated a unique id
   * To each experiment is associated a directory with raw data, processed data and metadata
   * Experiment metadata include:
       - a description of the hardware/os/vm/dbms setup
       - a snapshot of the state before the experiment
       - the output of the experiment
       - a trace of the experiment execution
       - a snapshot of the state after the experiment
    * The experiment is run in client/server mode
    * Experiment setup is controlled from the client.
    * Experiment results are stored on the client.
    * To each experiment is associated a set of scripts for setting up, running and cleaning up the experiment.  
    * The prerequisites on the server side are:
        - hypervisor installed with tuning VM and accessible via ssh/scp
        - account on the server accessible from client via ssh/scp
    * The Tuning VM contains:
        - DBMS accessible remotely via IP:port
        - tuning database can be created
        - account accessible from client via ssh/scp
        - a program _dbt_ is installed to run orchestrate the commands that are run during an including DML,fio, dtrace on the tuning VM
    * There is a template for creating a Tuning VM
    * There is a guideline for configuring client and server


## Experiments ##

Overall, the structure of an experiment is the following:
1. setup
2. run
3. cleanup

Before the actual setup, we capture the hw/os/vm/dbms setup info.

As there is no state on the server, the only prerequisite is that the DBMS is installed so that a database can be created. As a consequence:
    - the first action of the setup is to create a tuning database, and the last action of the cleanup is to drop that database. Both operations must be blocking.
    - all files needed on the server side are transferred from the client to the server. To avoid huge transfers, data is generated on the server side.
    - all files generated by an experiment are transferred back to the client.

Once the database is created, setup creates the appropriate tables (and indexes), generates data, and loads data (each of these steps is blocking). Note that it takes time to generate data and that several experiments might reuse the same data. A cache is used on the server in a location that can be reused across experiments. The file generated by the data generation tool is cached there. It is crucial that the name of this file identifies a data set, so that two experiments that need the same data set use the same name, and that the same name is not used to denote two different data sets.

An experiment run might include several repetitions with varying values for the experiment parameters, and with different repetitions for the same parameter. For each repetition, we consider the following steps:
    - run config: set parameter values 
    - get VM state snapshot 
    - run setup: create index, enforce cold/warm buffer.
        + this step is non trivial as it might require restarting the tuning VM. There is a need for a mechanism for resuming the experiment run when the tuning VM restarts.
    - start traces
    - run: execute experiment script 
    - stop traces
    - get VM snapshot
    - run cleanup: drop index

Once the experiment is finished and all relevant files have been transferred to the client, the tuning database is dropped and all files are deleted.

If an error occurs during an experiment, it is important to see if the database server is in a state where the running experiment (and following experiments) can continue or not. In any case, the error message is transferred back to the client.


### fio calibration ###
* quantify the performance of the i/o and file system on ssd
* find value for i/o stack tuning parameters in order to get as close as possible to data sheet performance.
* experimental methodology: 1 parameter varied at a time, 3 runs per experiments.

in:
- experiments: 4KB iops, seq read/write
- parameters: (nb threads, io size, queue depth), uFLIP params.

out:



